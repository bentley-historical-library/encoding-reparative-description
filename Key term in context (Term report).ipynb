{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ff37bc",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook uses the results from the `parse_resources.ipynb` notebook. The parse resources step pulls data from ArchivesSpace and creates a dataframe that was output to a CSV file. This notebook starts from the CSV file, but it could relatively easily be changed to take the previous dataframe as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58604bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import os\n",
    "import re\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97457b34",
   "metadata": {},
   "source": [
    "_Note:_ the following functions and code is based on work by Ella Li, who created an initial version of this project that parsed EAD data from XML files. The process here is similar but continues to use the data pulled from the ArchivesSpace API, which exports data in JSON rather than XML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216d25a",
   "metadata": {},
   "source": [
    "## Provide Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the txt file term list\n",
    "term_list_file = 'terms_all.txt'\n",
    "\n",
    "with open(term_list_file, 'r') as f:\n",
    "    terms = [line.strip() for line in f]\n",
    "\n",
    "print(f'Read term list from {term_list_file} and recorded {len(terms)} terms of interest.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a499b11",
   "metadata": {},
   "source": [
    "## Match Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce533da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def match_terms(row, terms, columns):\n",
    "    results = []\n",
    "    for term in terms:\n",
    "        for col in columns:\n",
    "            if not isinstance(row[col], float):\n",
    "                # split the column into paragraphs\n",
    "                # wonky try/except to work through integers, if not converted to strings\n",
    "                try:\n",
    "                    paragraphs = row[col].split('\\n')\n",
    "                except:\n",
    "                    paragraphs = str(row[col]).split('\\n')\n",
    "                # loop through each paragraph\n",
    "                for paragraph in paragraphs:\n",
    "                    # check if the term is in the current paragraph\n",
    "                    if re.search(r'\\b' + re.escape(term) + r'\\b', paragraph, re.IGNORECASE):\n",
    "                        # Split paragraph into sentences\n",
    "                        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', paragraph)\n",
    "                        # Find the sentence containing the term\n",
    "                        matched_sentence = next((sentence for sentence in sentences if re.search(r'\\b' + re.escape(term) + r'\\b', sentence, re.IGNORECASE)), paragraph)\n",
    "                        results.append({\n",
    "                            'Term': term,\n",
    "                            'Occurrence (ead_ID)': row['ead_id'],\n",
    "                            'Field': col, \n",
    "                            'Collection': row.get('titleproper', None),\n",
    "                            'Context': matched_sentence  # Returning only the matched sentence\n",
    "                        })\n",
    "                        \n",
    "    return results\n",
    "\n",
    "def match_and_visualize(df, name):\n",
    "    # Match results\n",
    "    results_df = pd.DataFrame([result for index, row in df.iterrows() for result in match_terms(row, terms, df.columns)])\n",
    "    \n",
    "    # Sort results by 'Term'\n",
    "    sorted_results_df = results_df.sort_values(by='Term', ascending=True)\n",
    "    \n",
    "    # Show matched results\n",
    "    print(\"Matched results for \", name)\n",
    "\n",
    "    # Export to CSV without the index\n",
    "    sorted_results_df.to_csv('matched_results_' + name + '.csv', index=False)\n",
    "    return sorted_results_df \n",
    "\n",
    "eads_df = pd.read_csv('results-fromTextFile.csv', encoding='utf-8')\n",
    "# eads_df = pd.read_csv('results-allIDs.csv', encoding='utf-8')\n",
    "\n",
    "term_report = match_and_visualize(eads_df, 'Bentley')\n",
    "match_and_visualize(eads_df, 'Bentley')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403e2f9",
   "metadata": {},
   "source": [
    "## Write Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbbefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Writing Results to CSV file')\n",
    "\n",
    "# df.to_csv('results-termReport-allIDs.csv', encoding='utf-8', index=False)\n",
    "term_report.to_csv('results-termReport-fromTextFile.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
