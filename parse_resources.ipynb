{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse ArchivesSpace Resources via the API\n",
    "\n",
    "### Import packages\n",
    "- configparser: Implements a basic configuration language which provides a structure you can use to write Python programs which can be customized by end users.\n",
    "- json: Exposes an API for JSON (JavaScript Object Notation).\n",
    "- requests: A HTTP library.\n",
    "- pandas: An open source data analysis and manipulation tool, built on top of the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration File\n",
    "\n",
    "In order to authenticate to ArchivesSpace and thus use the API, you'll have needed to supply a separate -- and ignored by git -- \"config.ini\" file in the home directory that looks like this:\n",
    "\n",
    "```\n",
    "[ARCHIVESSPACE]\n",
    "BaseURL = \n",
    "User = \n",
    "Password = \n",
    "Respository ID = \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Configuration File\n"
     ]
    }
   ],
   "source": [
    "print('Reading Configuration File')\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "base_url = config['ARCHIVESSPACE']['BaseURL']\n",
    "user = config['ARCHIVESSPACE']['User']\n",
    "password = config['ARCHIVESSPACE']['Password']\n",
    "repository_id = config['ARCHIVESSPACE']['RepositoryID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate to ArchivesSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating to ArchivesSpace\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print('Authenticating to ArchivesSpace')\n",
    "endpoint = '/users/' + user + '/login'\n",
    "params = {'password': password}\n",
    "response = requests.post(base_url + endpoint, params=params)\n",
    "print(response.status_code)\n",
    "\n",
    "response = response.json()\n",
    "session_key = response['session']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Resource IDs\n",
    "\n",
    "You can either get a list of _all_ resource IDs an an ArchivesSpace Repository, or you supply a separate \"resource_ids.txt\" file in the home directory with one line for every Resource ID for every Resource you want to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Resource IDs from text file.\n"
     ]
    }
   ],
   "source": [
    "resource_ids = []\n",
    "\n",
    "value = input('I want to parse all Resource IDs in ArchivesSpace. Enter True of False: ') or 'False'\n",
    "\n",
    "# Convert the input to a boolean value\n",
    "if value.lower() == \"true\":\n",
    "    bool_value = True\n",
    "elif value.lower() == \"false\":\n",
    "    bool_value = False\n",
    "else:\n",
    "    print(\"Invalid input. Please enter True or False.\")\n",
    "\n",
    "# Use the boolean value\n",
    "if bool_value:\n",
    "    print('Parsing all Resource Ids in ArchivesSpace.')\n",
    "    print('  - GETing Resource IDs')\n",
    "    endpoint = '/repositories/' + str(repository_id) + '/resources'\n",
    "    headers = {'X-ArchivesSpace-Session': session_key}\n",
    "    params = {'all_ids': True}\n",
    "    response = requests.get(base_url + endpoint, headers=headers, params=params)\n",
    "    print(response.status_code)\n",
    "\n",
    "    resource_ids = response.json()\n",
    "\n",
    "else:\n",
    "    print('Parsing Resource IDs from text file.')\n",
    "    with open('resource_ids.txt', mode='r') as f:\n",
    "        resource_ids = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for resource_id in resource_ids:\n",
    "    resource_id = resource_id.strip()\n",
    "\n",
    "    print('  - GETing Resource ' + str(resource_id))\n",
    "    endpoint = '/repositories/' + str(repository_id) + '/resources/' + str(resource_id)\n",
    "    headers = {'X-ArchivesSpace-Session': session_key}\n",
    "    response = requests.get(base_url + endpoint, headers=headers)\n",
    "    print(response.status_code)\n",
    "\n",
    "    resource = response.json()\n",
    "\n",
    "    ## extract id\n",
    "    eadid = resource['ead_id']\n",
    "\n",
    "    # Extract titleproper\n",
    "    titleproper = resource['finding_aid_title'][16:]\n",
    "\n",
    "    ## extract abstract\n",
    "    abstract = ''\n",
    "    for note in resource['notes']:\n",
    "        if note.get('type') == 'abstract':\n",
    "            abstract = note['content'][0]\n",
    "                \n",
    "    ## Extract language\n",
    "    language = resource['finding_aid_language_note'].replace('<language encodinganalog=\"Language\" langcode=\"eng\">English.</language>', 'English.')\n",
    "\n",
    "    ## Extract scopecontent\n",
    "    scopecontent = ''\n",
    "    for note in resource['notes']:\n",
    "        if note.get('type') == 'scopecontent':\n",
    "            scopecontent = note['subnotes'][0]['content']\n",
    "                \n",
    "    ## Extract bioghist    \n",
    "    bioghist = ''\n",
    "    for note in resource['notes']:\n",
    "        if note.get('type') == 'bioghist':\n",
    "            bioghist = note['subnotes'][0].get('content', '')\n",
    "                \n",
    "    ## Extract custodhist   \n",
    "\n",
    "    ## Extract controlaccess\n",
    "    subject_ids = []\n",
    "    subjects = []\n",
    "    subjects_source = []\n",
    "\n",
    "    genreform_ids = []\n",
    "    genreforms = []\n",
    "    genreforms_source = []\n",
    "\n",
    "    geogname_ids = []\n",
    "    geognames = []\n",
    "    geognames_source = []\n",
    "\n",
    "    for subject in resource['subjects']:\n",
    "        subject_id = subject['ref'].split('/')[-1]\n",
    "            \n",
    "        print('  - GETing Subject ' + str(subject_id))\n",
    "        endpoint = '/subjects/' + str(subject_id)\n",
    "        response = requests.get(base_url + endpoint, headers=headers)\n",
    "        print(response.status_code)\n",
    "            \n",
    "        subject = response.json()\n",
    "            \n",
    "        if subject['terms'][0]['term_type'] == 'topical':\n",
    "            subject_ids.append(subject_id)\n",
    "            subjects.append(subject['terms'][0]['term'])\n",
    "            subjects_source.append(subject.get('source', 'No Source'))\n",
    "            \n",
    "        if subject['terms'][0]['term_type'] == 'genre_form':\n",
    "            genreform_ids.append(subject_id)\n",
    "            genreforms.append(subject['terms'][0]['term'])\n",
    "            genreforms_source.append(subject.get('source', 'No Source'))\n",
    "            \n",
    "        if subject['terms'][0]['term_type'] == 'geographic':\n",
    "            geogname_ids.append(subject_id)\n",
    "            geognames.append(subject['terms'][0]['term'])\n",
    "            geognames_source.append(subject.get('source', 'No Source'))\n",
    "\n",
    "    persname_ids = []\n",
    "    persnames = []\n",
    "    persnames_source = []\n",
    "\n",
    "    corpname_ids = []\n",
    "    corpnames = []\n",
    "    corpnames_source = []\n",
    "\n",
    "    famname_ids = []\n",
    "    famnames = []\n",
    "    famnames_source = []\n",
    "\n",
    "    for linked_agent in resource['linked_agents']:\n",
    "        linked_agent_id = linked_agent['ref'].split('/')[-1]\n",
    "            \n",
    "        if 'people' in linked_agent['ref']:\n",
    "            print('  - GETing Person Agent ' + str(linked_agent_id))\n",
    "            endpoint = '/agents/people/' + str(linked_agent_id)\n",
    "            response = requests.get(base_url + endpoint, headers=headers)\n",
    "            print(response.status_code)\n",
    "            \n",
    "            person_agent = response.json()\n",
    "            persname_ids.append(linked_agent_id)\n",
    "            persnames.append(person_agent['names'][0]['sort_name'])\n",
    "            persnames_source.append(person_agent['names'][0].get('source', 'No Source'))\n",
    "                \n",
    "        if 'corporate_entities' in linked_agent['ref']:\n",
    "            print('  - GETing Coporate Entity Agent ' + str(linked_agent_id))\n",
    "            endpoint = '/agents/corporate_entities/' + str(linked_agent_id)\n",
    "            response = requests.get(base_url + endpoint, headers=headers)\n",
    "            print(response.status_code)\n",
    "            \n",
    "            corporate_entity_agent = response.json()\n",
    "            corpname_ids.append(linked_agent_id)\n",
    "            corpnames.append(corporate_entity_agent['names'][0]['sort_name'])\n",
    "            corpnames_source.append(corporate_entity_agent['names'][0].get('source', 'No Source'))\n",
    "                \n",
    "        if 'families' in linked_agent['ref']:\n",
    "            print('  - GETing Family Agent ' + str(linked_agent_id))\n",
    "            endpoint = '/agents/families/' + str(linked_agent_id)\n",
    "            response = requests.get(base_url + endpoint, headers=headers)\n",
    "            print(response.status_code)\n",
    "            \n",
    "            family_agent = response.json()\n",
    "            famname_ids.append(linked_agent_id)\n",
    "            famnames.append(family_agent['names'][0]['sort_name'])\n",
    "            famnames_source.append(family_agent['names'][0].get('source', 'No Source'))\n",
    "                \n",
    "    result = [str(resource_id), \n",
    "              eadid, \n",
    "              titleproper, \n",
    "              abstract, \n",
    "              language, \n",
    "              scopecontent, \n",
    "              bioghist, \n",
    "              '; '.join(subject_ids), \n",
    "              '; '.join(subjects), \n",
    "              '; '.join(subjects_source), \n",
    "              '; '.join(genreform_ids), \n",
    "              '; '.join(genreforms), \n",
    "              '; '.join(genreforms_source), \n",
    "              '; '.join(geogname_ids), \n",
    "              '; '.join(geognames), \n",
    "              '; '.join(geognames_source), \n",
    "              '; '.join(persname_ids), \n",
    "              '; '.join(persnames), \n",
    "              '; '.join(persnames_source), \n",
    "              '; '.join(corpname_ids), \n",
    "              '; '.join(corpnames), \n",
    "              '; '.join(corpnames_source), \n",
    "              '; '.join(famname_ids), \n",
    "              '; '.join(famnames), \n",
    "              '; '.join(famnames_source)]\n",
    "    results.append(result)\n",
    "\n",
    "# Create the pandas DataFrame \n",
    "df = pd.DataFrame(results, columns = ['resource_id',\n",
    "                                      'ead_id', \n",
    "                                      'titleproper', \n",
    "                                      'abstract', \n",
    "                                      'language', \n",
    "                                      'scopecontent', \n",
    "                                      'bioghist', \n",
    "                                      'subject_ids',\n",
    "                                      'subjects', \n",
    "                                      'subjects_source', \n",
    "                                      'genreform_ids',\n",
    "                                      'genreforms', \n",
    "                                      'genreforms_source', \n",
    "                                      'geogname_ids',\n",
    "                                      'geognames', \n",
    "                                      'geognames_source', \n",
    "                                      'persname_ids',\n",
    "                                      'persnames', \n",
    "                                      'persnames_source', \n",
    "                                      'corpname_ids',\n",
    "                                      'corpnames', \n",
    "                                      'corpnames_source', \n",
    "                                      'famname_ids',\n",
    "                                      'famnames', \n",
    "                                      'famnames_source']) \n",
    "\n",
    "print(\"Alright, we're done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Writing Results to CSV file')\n",
    "\n",
    "\n",
    "if bool_value:\n",
    "    df.to_csv('results-allIDs.csv', encoding='utf-8', index=False)\n",
    "else:\n",
    "    df.to_csv('results-fromTextFile.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
