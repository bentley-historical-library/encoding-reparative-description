{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse ArchivesSpace Resources via the API\n",
    "\n",
    "### Import packages\n",
    "- configparser: Implements a basic configuration language which provides a structure you can use to write Python programs which can be customized by end users.\n",
    "- json: Exposes an API for JSON (JavaScript Object Notation).\n",
    "- requests: A HTTP library.\n",
    "- pandas: An open source data analysis and manipulation tool, built on top of the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration File\n",
    "\n",
    "In order to authenticate to ArchivesSpace and thus use the API, you'll have needed to supply a separate -- and ignored by git -- \"config.ini\" file in the home directory that looks like this:\n",
    "\n",
    "```\n",
    "[ARCHIVESSPACE]\n",
    "BaseURL = \n",
    "User = \n",
    "Password = \n",
    "Respository ID = \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Configuration File\n"
     ]
    }
   ],
   "source": [
    "print('Reading Configuration File')\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "base_url = config['ARCHIVESSPACE']['BaseURL']\n",
    "user = config['ARCHIVESSPACE']['User']\n",
    "password = config['ARCHIVESSPACE']['Password']\n",
    "repository_id = config['ARCHIVESSPACE']['RepositoryID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate to ArchivesSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating to ArchivesSpace\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print('Authenticating to ArchivesSpace')\n",
    "endpoint = '/users/' + user + '/login'\n",
    "params = {'password': password}\n",
    "response = requests.post(base_url + endpoint, params=params)\n",
    "print(response.status_code)\n",
    "\n",
    "response = response.json()\n",
    "session_key = response['session']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Resource IDs from Text File\n",
    "\n",
    "In order to know which ArchivesSpace Resources to parse, you'll have needed to supply a separate \"resource_ids.txt\" file in the home directory with one line for every Resource ID for every Resource you want to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Resource IDs from Text File\n"
     ]
    }
   ],
   "source": [
    "print('Reading Resource IDs from Text File')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Resources\n",
      "  - GETing Resource 3011\n",
      "200\n",
      "  - GETing Subject 3452\n",
      "200\n",
      "  - GETing Subject 6593\n",
      "200\n",
      "  - GETing Subject 11286\n",
      "200\n",
      "  - GETing Subject 945\n",
      "200\n",
      "  - GETing Subject 3469\n",
      "200\n",
      "  - GETing Subject 16557\n",
      "200\n",
      "  - GETing Person Agent 12338\n",
      "200\n",
      "  - GETing Person Agent 12338\n",
      "200\n",
      "  - GETing Family Agent 496\n",
      "200\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'family_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 131\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m    130\u001b[0m family_agent \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m--> 131\u001b[0m \u001b[43mfamily_ids\u001b[49m\u001b[38;5;241m.\u001b[39mappend(linked_agent_id)\n\u001b[0;32m    132\u001b[0m famnames\u001b[38;5;241m.\u001b[39mappend(family_agent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msort_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    133\u001b[0m famnames_source\u001b[38;5;241m.\u001b[39mappend(family_agent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Source\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'family_ids' is not defined"
     ]
    }
   ],
   "source": [
    "print('Parsing Resources')\n",
    "# This whole bit could be refactored...\n",
    "\n",
    "results = []\n",
    "\n",
    "with open('resource_ids.txt', mode='r') as f:\n",
    "    resource_ids = f.readlines()\n",
    "    \n",
    "    for resource_id in resource_ids:\n",
    "        resource_id = resource_id.strip()\n",
    "\n",
    "        print('  - GETing Resource ' + str(resource_id))\n",
    "        endpoint = '/repositories/' + str(repository_id) + '/resources/' + str(resource_id)\n",
    "        headers = {'X-ArchivesSpace-Session': session_key}\n",
    "        response = requests.get(base_url + endpoint, headers=headers)\n",
    "        print(response.status_code)\n",
    "\n",
    "        resource = response.json()\n",
    "\n",
    "        ## extract id\n",
    "        eadid = resource['ead_id']\n",
    "\n",
    "        # Extract titleproper\n",
    "        titleproper = resource['finding_aid_title'][16:]\n",
    "\n",
    "        ## extract abstract\n",
    "        abstract = ''\n",
    "        for note in resource['notes']:\n",
    "            if note.get('type') == 'abstract':\n",
    "                abstract = note['content'][0]\n",
    "                \n",
    "        ## Extract language\n",
    "        language = resource['finding_aid_language_note'].replace('<language encodinganalog=\"Language\" langcode=\"eng\">English.</language>', 'English.')\n",
    "\n",
    "        ## Extract scopecontent\n",
    "        scopecontent = ''\n",
    "        for note in resource['notes']:\n",
    "            if note.get('type') == 'scopecontent':\n",
    "                scopecontent = note['subnotes'][0]['content']\n",
    "                \n",
    "        ## Extract bioghist    \n",
    "        bioghist = ''\n",
    "        for note in resource['notes']:\n",
    "            if note.get('type') == 'bioghist':\n",
    "                bioghist = note['subnotes'][0].get('content', '')\n",
    "                \n",
    "        ## Extract custodhist   \n",
    "\n",
    "         ## Extract controlaccess\n",
    "        subject_ids = []\n",
    "        subjects = []\n",
    "        subjects_source = []\n",
    "\n",
    "        genreform_ids = []\n",
    "        genreforms = []\n",
    "        genreforms_source = []\n",
    "\n",
    "        geogname_ids = []\n",
    "        geognames = []\n",
    "        geognames_source = []\n",
    "\n",
    "        for subject in resource['subjects']:\n",
    "            subject_id = subject['ref'].split('/')[-1]\n",
    "            \n",
    "            print('  - GETing Subject ' + str(subject_id))\n",
    "            endpoint = '/subjects/' + str(subject_id)\n",
    "            response = requests.get(base_url + endpoint, headers=headers)\n",
    "            print(response.status_code)\n",
    "            \n",
    "            subject = response.json()\n",
    "            \n",
    "            if subject['terms'][0]['term_type'] == 'topical':\n",
    "                subject_ids.append(subject_id)\n",
    "                subjects.append(subject['terms'][0]['term'])\n",
    "                subjects_source.append(subject.get('source', 'No Source'))\n",
    "            \n",
    "            if subject['terms'][0]['term_type'] == 'genre_form':\n",
    "                genreform_ids.append(subject_id)\n",
    "                genreforms.append(subject['terms'][0]['term'])\n",
    "                genreforms_source.append(subject.get('source', 'No Source'))\n",
    "            \n",
    "            if subject['terms'][0]['term_type'] == 'geographic':\n",
    "                geogname_ids.append(subject_id)\n",
    "                geognames.append(subject['terms'][0]['term'])\n",
    "                geognames_source.append(subject.get('source', 'No Source'))\n",
    "\n",
    "        persname_ids = []\n",
    "        persnames = []\n",
    "        persnames_source = []\n",
    "\n",
    "        corpname_ids = []\n",
    "        corpnames = []\n",
    "        corpnames_source = []\n",
    "\n",
    "        famname_ids = []\n",
    "        famnames = []\n",
    "        famnames_source = []\n",
    "\n",
    "        for linked_agent in resource['linked_agents']:\n",
    "            linked_agent_id = linked_agent['ref'].split('/')[-1]\n",
    "            \n",
    "            if 'people' in linked_agent['ref']:\n",
    "                print('  - GETing Person Agent ' + str(linked_agent_id))\n",
    "                endpoint = '/agents/people/' + str(linked_agent_id)\n",
    "                response = requests.get(base_url + endpoint, headers=headers)\n",
    "                print(response.status_code)\n",
    "            \n",
    "                person_agent = response.json()\n",
    "                persname_ids.append(linked_agent_id)\n",
    "                persnames.append(person_agent['names'][0]['sort_name'])\n",
    "                persnames_source.append(person_agent['names'][0].get('source', 'No Source'))\n",
    "                \n",
    "            if 'corporate_entities' in linked_agent['ref']:\n",
    "                print('  - GETing Coporate Entity Agent ' + str(linked_agent_id))\n",
    "                endpoint = '/agents/corporate_entities/' + str(linked_agent_id)\n",
    "                response = requests.get(base_url + endpoint, headers=headers)\n",
    "                print(response.status_code)\n",
    "            \n",
    "                corporate_entity_agent = response.json()\n",
    "                corpname_ids.append(linked_agent_id)\n",
    "                corpnames.append(corporate_entity_agent['names'][0]['sort_name'])\n",
    "                corpnames_source.append(corporate_entity_agent['names'][0].get('source', 'No Source'))\n",
    "                \n",
    "            if 'families' in linked_agent['ref']:\n",
    "                print('  - GETing Family Agent ' + str(linked_agent_id))\n",
    "                endpoint = '/agents/families/' + str(linked_agent_id)\n",
    "                response = requests.get(base_url + endpoint, headers=headers)\n",
    "                print(response.status_code)\n",
    "            \n",
    "                family_agent = response.json()\n",
    "                famname_ids.append(linked_agent_id)\n",
    "                famnames.append(family_agent['names'][0]['sort_name'])\n",
    "                famnames_source.append(family_agent['names'][0].get('source', 'No Source'))\n",
    "                \n",
    "        result = [str(resource_id), \n",
    "                  eadid, \n",
    "                  titleproper, \n",
    "                  abstract, \n",
    "                  language, \n",
    "                  scopecontent, \n",
    "                  bioghist, \n",
    "                  '; '.join(subject_ids), \n",
    "                  '; '.join(subjects), \n",
    "                  '; '.join(subjects_source), \n",
    "                  '; '.join(genreform_ids), \n",
    "                  '; '.join(genreforms), \n",
    "                  '; '.join(genreforms_source), \n",
    "                  '; '.join(geogname_ids), \n",
    "                  '; '.join(geognames), \n",
    "                  '; '.join(geognames_source), \n",
    "                  '; '.join(persname_ids), \n",
    "                  '; '.join(persnames), \n",
    "                  '; '.join(persnames_source), \n",
    "                  '; '.join(corpname_ids), \n",
    "                  '; '.join(corpnames), \n",
    "                  '; '.join(corpnames_source), \n",
    "                  '; '.join(famname_ids), \n",
    "                  '; '.join(famnames), \n",
    "                  '; '.join(famnames_source)]\n",
    "        results.append(result)\n",
    "\n",
    "# Create the pandas DataFrame \n",
    "df = pd.DataFrame(results, columns = ['resource_id',\n",
    "                                      'ead_id', \n",
    "                                      'titleproper', \n",
    "                                      'abstract', \n",
    "                                      'language', \n",
    "                                      'scopecontent', \n",
    "                                      'bioghist', \n",
    "                                      'subject_ids',\n",
    "                                      'subjects', \n",
    "                                      'subjects_source', \n",
    "                                      'genreform_ids',\n",
    "                                      'genreforms', \n",
    "                                      'genreforms_source', \n",
    "                                      'geogname_ids',\n",
    "                                      'geognames', \n",
    "                                      'geognames_source', \n",
    "                                      'persname_ids',\n",
    "                                      'persnames', \n",
    "                                      'persnames_source', \n",
    "                                      'corpname_ids',\n",
    "                                      'corpnames', \n",
    "                                      'corpnames_source', \n",
    "                                      'famname_ids',\n",
    "                                      'famnames', \n",
    "                                      'famnames_source']) \n",
    "\n",
    "print(\"Alright, we're done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Results to CSV file\n"
     ]
    }
   ],
   "source": [
    "print('Writing Results to CSV file')\n",
    "\n",
    "df.to_csv('results.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
